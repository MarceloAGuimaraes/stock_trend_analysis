{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction\n",
    "\n",
    "In this Notebook, we will create the actual prediction system, by testing various approaches and accuracy against multiple time-horizons (target_days variable).\n",
    "\n",
    "First we will load all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "sys.path.insert(1, '..')\n",
    "import recommender as rcmd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# classification approaches\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# regression approaches\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data handling and scoring\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the input data pipelines for stock and statement data. Therefore we will have to split data into training and test sets. There are two options for doing that:\n",
    "\n",
    "* Splitting the list of symbols\n",
    "* Splitting the results list of training stock datapoints\n",
    "\n",
    "We will use the first option in order ensure a clear split (since the generate data has overlapping time frames, the second options would generate data that might have been seen by the system beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  -0.25  0.    0.25  0.5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\recommender\\learning\\preprocess.py:259: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:318: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['pe_ratio'] = np.divide(df[col_price], df['eps_diluted'])\n",
      "..\\recommender\\learning\\preprocess.py:319: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['cash_share'] = np.divide(df['cash_net'], np.divide(df['shareholder_equity'], df[col_price]))\n",
      "..\\recommender\\stocks\\Statements.py:66: UserWarning: No statements found for given symbols\n",
      "  warnings.warn(\"No statements found for given symbols\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1b969c864e83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m22\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-1b969c864e83>\u001b[0m in \u001b[0;36mtrain_test_data\u001b[1;34m(back, ahead, xlim, split, count, stocks, cache)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# generate sample data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mahead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrcmd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mahead\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxlim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programming\\github\\udacity\\stock_trend_analysis\\recommender\\learning\\preprocess.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(symbols, stocks, cache, back, ahead, xlim, num_cats, jump_size, smooth, sm, ti)\u001b[0m\n\u001b[0;32m    364\u001b[0m   \u001b[1;31m# load relevant stocks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mti\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mti\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAlphaVantageTicker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m   \u001b[0mdf_stocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_stock_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m   \u001b[1;31m# preprocess the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programming\\github\\udacity\\stock_trend_analysis\\recommender\\stocks\\Cache.py\u001b[0m in \u001b[0;36mload_stock_data\u001b[1;34m(self, symbols, stocks, ticker, cache, load_data)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;31m# combine and return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_stocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mload_statement_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programming\\envs\\ds-stocks\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    226\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m                        copy=copy, sort=sort)\n\u001b[0m\u001b[0;32m    229\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programming\\envs\\ds-stocks\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No objects to concatenate'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# create cache object\n",
    "cache = rcmd.stocks.Cache()\n",
    "\n",
    "# load list of all available stocks and sample sub-list\n",
    "stocks = cache.list_data('stock')\n",
    "\n",
    "def train_test_data(back, ahead, xlim, split=0.3, count=2000, stocks=stocks, cache=cache):\n",
    "    '''Generetes a train test split'''\n",
    "    sample = np.random.choice(list(stocks.keys()), 2000)\n",
    "    # split the stock data\n",
    "    count_train = int((1-split) * count)\n",
    "    sample_train = sample[:count_train]\n",
    "    sample_test = sample[count_train:]\n",
    "\n",
    "    # generate sample data\n",
    "    df_train = rcmd.learning.preprocess.create_dataset(sample_train, stocks, cache, back, ahead, xlim)\n",
    "    df_test = rcmd.learning.preprocess.create_dataset(sample_test, stocks, cache, back, ahead, xlim)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_data(10, 22, (-.5, .5), split=0.2, count=4000)\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortcut: store / load created datasets\n",
    "df_train.to_csv('../data/train.csv')\n",
    "df_test.to_csv('../data/test.csv')\n",
    "\n",
    "# load data\n",
    "#df_train = pd.read_csv('../data/train.csv')\n",
    "#df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded and split the data, we have to divide it into input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_data(df, xlim, balance_mode=None, balance_weight=1):\n",
    "    '''Splits the data into 3 sets: input, ouput_classify, output_regression.\n",
    "    \n",
    "    Note that this function will also sample the data if choosen to create a more balanced dataset. Options are:\n",
    "        `under`: Undersamples the data (takes lowest data sample and )\n",
    "        `over`: Oversamples data to the highest number of possible samples\n",
    "        `over_under`: takes the mean count and samples in both directions\n",
    "        \n",
    "    Args:\n",
    "        df (DataFrame): DF to contain all relevant data\n",
    "        xlim (tuple): tuple of integers used to clip and scale regression values to a range of 0 to 1\n",
    "        balance_mode (str): Defines the balance mode of the data (options: 'over_under', 'under', 'over')\n",
    "        balance_weight (float): Defines how much the calculate sample count is weighted in comparision to the actual count (should be between 0 and 1)\n",
    "        \n",
    "    Returns:\n",
    "        df_X: DataFrame with input values\n",
    "        df_y_cls: DataFrame with classification labels\n",
    "        df_y_reg: DataFrame with regression values\n",
    "    '''\n",
    "    # sample the data correctly\n",
    "    if balance_mode is not None:\n",
    "        if balance_mode == 'over_under':\n",
    "            # find median\n",
    "            num_samples = df['target_cat'].value_counts().median().astype('int')\n",
    "        elif balance_mode == 'over':\n",
    "            # find highest number\n",
    "            num_samples = df['target_cat'].value_counts().max()\n",
    "        elif balance_mode == 'under':\n",
    "            # find minimal number\n",
    "            num_samples = df['target_cat'].value_counts().min()\n",
    "        else:\n",
    "            raise ValueError('Unkown sample mode: {}'.format(balance_mode))\n",
    "        # sample categories\n",
    "        dfs = []\n",
    "        for cat in df['target_cat'].unique():\n",
    "            df_tmp = df[df['target_cat'] == cat]\n",
    "            cur_samples = int(balance_weight * num_samples + (1-balance_weight) * df_tmp.shape[0])\n",
    "            sample = df_tmp.sample(cur_samples, replace=cur_samples > df_tmp.shape[0])\n",
    "            dfs.append(sample)\n",
    "        # concat and shuffle the rows\n",
    "        df = pd.concat(dfs, axis=0).sample(frac=1)\n",
    "    \n",
    "    # remove all target cols\n",
    "    df_X = df.drop(['target', 'target_cat', 'norm_price', 'symbol'], axis=1)\n",
    "    # convert to dummy classes\n",
    "    df_y_cls = pd.get_dummies(df['target_cat'], prefix='cat', dummy_na=False)\n",
    "    # clip values and scale to vals\n",
    "    df_y_reg = np.divide( np.subtract( df['target'].clip(xlim[0], xlim[1]), xlim[0] ), (xlim[1] - xlim[0]) )\n",
    "    \n",
    "    return df, df_X, df_y_cls, df_y_reg\n",
    "\n",
    "df_train_bm, X_train, y_ctrain, y_rtrain = divide_data(df_train, (-.5, .5), balance_mode='over_under', balance_weight=0.9)\n",
    "df_test_bm, X_test, y_ctest, y_rtest = divide_data(df_test, (-.5, .5))\n",
    "print(pd.concat([y_ctrain.sum(axis=0), y_ctest.sum(axis=0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create the actual prediction systems, we will have to define metrics, how we want to measure the success of the systems.\n",
    "As we have two approaches (classification and regression) we will use two types metrics:\n",
    "\n",
    "* Precision, Recall & Accuracy\n",
    "* RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric_classifier(y_true, y_pred, avg=None):\n",
    "    p = precision_score(y_true, y_pred, average=avg)\n",
    "    r = recall_score(y_true, y_pred, average=avg)\n",
    "    f1 = f1_score(y_true, y_pred, average=avg)\n",
    "    return f1, p, r\n",
    "\n",
    "def score_classifier(y_true, y_pred):\n",
    "    '''Calculates the relevant scores for a classifer and outputs. This should show predicitions per class.'''\n",
    "    f1, p, r = _metric_classifier(y_true, y_pred, avg='micro')\n",
    "    \n",
    "    print(\"Model Performance: F1={:.4f} (P={:.4f} / R={:.4f})\".format(f1, p, r))\n",
    "    # list scores of single classes\n",
    "    for i, c in enumerate(y_true.columns):\n",
    "        sf1, sp, sr = _metric_classifier(y_true.iloc[:, i], y_pred[:, i], avg='binary')\n",
    "        print(\"  {:10} F1={:.4f} (P={:.4f} / R={:.4f})\".format(c + \":\", sf1, sp, sr))\n",
    "        \n",
    "def score_regression(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(\"Model Performance: MSE={:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The first step is to create a baseline for both approaches (classification and regression). In case of regression our target value will be `target` and for classification it will be `target_cat` (which we might convert into a one-hot vector along the way).\n",
    "\n",
    "Lets start with the simpler form of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_0    2818\n",
       "cat_1    2818\n",
       "cat_2    2818\n",
       "cat_3    2818\n",
       "cat_4    2818\n",
       "cat_5    2818\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ctrain.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale input data to improve convergance (Note: scaler has to be used for other input data as well)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# train element\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=500, solver='lbfgs'))\n",
    "classifier.fit(X_train_std, y_ctrain)\n",
    "\n",
    "# predict data\n",
    "y_pred = classifier.predict(X_test_std)\n",
    "\n",
    "score_classifier(y_ctest, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a strong bias in the system for `cat_3`, which also has the highest number of training samples. Future work might include oversampling or more target datapoint selection to reduce these biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, support vector machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: F1=0.4157 (P=0.5416 / R=0.3372)\n",
      "  cat_0:     F1=0.0385 (P=1.0000 / R=0.0196)\n",
      "  cat_1:     F1=0.0154 (P=1.0000 / R=0.0078)\n",
      "  cat_2:     F1=0.0107 (P=0.4932 / R=0.0054)\n",
      "  cat_3:     F1=0.6126 (P=0.5414 / R=0.7053)\n",
      "  cat_4:     F1=0.0027 (P=1.0000 / R=0.0014)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_svm = MultiOutputClassifier(SVC())\n",
    "classifier_svm.fit(X_train_std, y_ctrain)\n",
    "y_pred_svm = classifier_svm.predict(X_test_std)\n",
    "score_classifier(y_ctest, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, data=X_test_std):\n",
    "        self.data = data\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss, acc = self.model.evaluate(self.data, df_test_bm['target_cat'].to_numpy(), verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple feed forward network\n",
    "print(X_train.shape)\n",
    "print(df_train.shape)\n",
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train.to_numpy(), df_train_bm['target_cat'].to_numpy(), epochs=100, callbacks=[TestCallback()])\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test.to_numpy())\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that the output of the model in the test data resembles the input distribution. Lets try to improve generalization with a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = tf.keras.layers.PReLU\n",
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(32), act(),\n",
    "    tf.keras.layers.Dense(64), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(128), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64), act(),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train.to_numpy(), df_train_bm['target_cat'].to_numpy(), epochs=200, callbacks=[TestCallback(X_test.to_numpy())])\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test.to_numpy())\n",
    "print(y_pred_ffn)\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "classifier_ffn.save('../data/keras-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "The other possible option is regression. We will test a linear regression against neural networks based on RMSE score to see how the predictions hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: MSE=0.0329\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train.iloc[:, :7].to_numpy(), y_rtrain)\n",
    "y_pred_reg = reg.predict(X_test.iloc[:, :7].to_numpy())\n",
    "score_regression(y_rtest, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42542 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6332BFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6332BFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 8s 181us/sample - loss: 72308.8231 - accuracy: 0.0097\n",
      "Epoch 2/20\n",
      "42542/42542 [==============================] - 6s 146us/sample - loss: 3158.4172 - accuracy: 0.0091\n",
      "Epoch 3/20\n",
      "42542/42542 [==============================] - 7s 155us/sample - loss: 7.5809 - accuracy: 0.0061\n",
      "Epoch 4/20\n",
      "42542/42542 [==============================] - 7s 156us/sample - loss: 5.0027 - accuracy: 0.0062\n",
      "Epoch 5/20\n",
      "42542/42542 [==============================] - 6s 151us/sample - loss: 4.0884 - accuracy: 0.0061\n",
      "Epoch 6/20\n",
      "42542/42542 [==============================] - 7s 161us/sample - loss: 3.5077 - accuracy: 0.0063\n",
      "Epoch 7/20\n",
      "42542/42542 [==============================] - 6s 140us/sample - loss: 2.9895 - accuracy: 0.0062\n",
      "Epoch 8/20\n",
      "42542/42542 [==============================] - 7s 165us/sample - loss: 2.7040 - accuracy: 0.0061\n",
      "Epoch 9/20\n",
      "42542/42542 [==============================] - 7s 169us/sample - loss: 2.4751 - accuracy: 0.0061\n",
      "Epoch 10/20\n",
      "42542/42542 [==============================] - 7s 156us/sample - loss: 2.2585 - accuracy: 0.0062\n",
      "Epoch 11/20\n",
      "42542/42542 [==============================] - 6s 138us/sample - loss: 2.0987 - accuracy: 0.0061\n",
      "Epoch 12/20\n",
      "42542/42542 [==============================] - 6s 136us/sample - loss: 1.9689 - accuracy: 0.0061\n",
      "Epoch 13/20\n",
      "42542/42542 [==============================] - 6s 150us/sample - loss: 1.8492 - accuracy: 0.0061\n",
      "Epoch 14/20\n",
      "42542/42542 [==============================] - 7s 169us/sample - loss: 1.7474 - accuracy: 0.0060\n",
      "Epoch 15/20\n",
      "42542/42542 [==============================] - 8s 186us/sample - loss: 1.6806 - accuracy: 0.0061\n",
      "Epoch 16/20\n",
      "42542/42542 [==============================] - 7s 167us/sample - loss: 1.6234 - accuracy: 0.0061\n",
      "Epoch 17/20\n",
      "42542/42542 [==============================] - 6s 148us/sample - loss: 1.5783 - accuracy: 0.0060\n",
      "Epoch 18/20\n",
      "42542/42542 [==============================] - 7s 159us/sample - loss: 1.5221 - accuracy: 0.0060\n",
      "Epoch 19/20\n",
      "42542/42542 [==============================] - 6s 141us/sample - loss: 1.4846 - accuracy: 0.0060\n",
      "Epoch 20/20\n",
      "42542/42542 [==============================] - 6s 143us/sample - loss: 1.4333 - accuracy: 0.0061\n",
      "Model Performance: MSE=1.5827\n"
     ]
    }
   ],
   "source": [
    "classifier_reg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.00000001, nesterov=False)\n",
    "classifier_reg.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "classifier_reg.fit(X_train.to_numpy(), y_rtrain.to_numpy(), epochs=20)\n",
    "\n",
    "y_pred_reg = classifier_reg.predict(X_test.to_numpy())\n",
    "score_regression(y_rtest, y_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: MSE=3.2631\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10986691],\n",
       "       [ 0.23991095],\n",
       "       [-0.34694692],\n",
       "       ...,\n",
       "       [ 0.0956597 ],\n",
       "       [-0.04288995],\n",
       "       [ 0.15074253]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18622, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00190905 0.0575433  0.4032011  0.48665118 0.04683916 0.00385619]\n",
      " [0.00347802 0.07319234 0.39260495 0.47064242 0.05408182 0.00600046]\n",
      " [0.00484556 0.07138    0.38683167 0.45450288 0.06774367 0.01469623]\n",
      " ...\n",
      " [0.00111771 0.03497788 0.40016484 0.5337459  0.0289243  0.00106938]\n",
      " [0.00483004 0.07679388 0.38836578 0.45680222 0.06269442 0.01051365]\n",
      " [0.00092138 0.0323066  0.41052336 0.5202334  0.03445359 0.00156169]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_ffn = classifier_ffn.predict(X_test.to_numpy())\n",
    "print(y_pred_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-stocks",
   "language": "python",
   "name": "ds-stocks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
