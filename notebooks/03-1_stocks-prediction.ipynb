{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Value Prediction\n",
    "\n",
    "In this Notebook, we will create the actual prediction system, by testing various approaches and accuracy against multiple time-horizons (target_days variable).\n",
    "\n",
    "First we will load all libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "sys.path.insert(1, '..')\n",
    "import recommender as rcmd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# classification approaches\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# regression approaches\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data handling and scoring\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the input data pipelines for stock and statement data. Therefore we will have to split data into training and test sets. There are two options for doing that:\n",
    "\n",
    "* Splitting the list of symbols\n",
    "* Splitting the results list of training stock datapoints\n",
    "\n",
    "We will use the first option in order ensure a clear split (since the generate data has overlapping time frames, the second options would generate data that might have been seen by the system beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  -0.25  0.    0.25  0.5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: invalid value encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:311: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['pe_ratio'] = np.divide(df[col_price], df['eps_diluted'])\n",
      "..\\recommender\\learning\\preprocess.py:312: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['cash_share'] = np.divide(df['cash_net'], np.divide(df['shareholder_equity'], df[col_price]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.5  -0.25  0.    0.25  0.5 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\recommender\\learning\\preprocess.py:252: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['expenses_research_netcash'] = np.divide(df['expenses_research'], df['cash_net'])\n",
      "..\\recommender\\learning\\preprocess.py:311: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['pe_ratio'] = np.divide(df[col_price], df['eps_diluted'])\n",
      "..\\recommender\\learning\\preprocess.py:312: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  df['cash_share'] = np.divide(df['cash_net'], np.divide(df['shareholder_equity'], df[col_price]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42542, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_1</th>\n",
       "      <th>day_2</th>\n",
       "      <th>day_3</th>\n",
       "      <th>day_4</th>\n",
       "      <th>day_5</th>\n",
       "      <th>day_6</th>\n",
       "      <th>day_7</th>\n",
       "      <th>day_8</th>\n",
       "      <th>day_9</th>\n",
       "      <th>day_10</th>\n",
       "      <th>...</th>\n",
       "      <th>dividend_share_growth_10y</th>\n",
       "      <th>revenue_share_growth_3y</th>\n",
       "      <th>revenue_share_growth_5y</th>\n",
       "      <th>revenue_share_growth_10y</th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>cash_share</th>\n",
       "      <th>norm_price</th>\n",
       "      <th>target</th>\n",
       "      <th>target_cat</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.074448</td>\n",
       "      <td>-0.032145</td>\n",
       "      <td>-0.034584</td>\n",
       "      <td>-0.060832</td>\n",
       "      <td>-0.066147</td>\n",
       "      <td>-0.055590</td>\n",
       "      <td>-0.081219</td>\n",
       "      <td>-0.063635</td>\n",
       "      <td>-0.052241</td>\n",
       "      <td>-0.030580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>40.395588</td>\n",
       "      <td>37.358140</td>\n",
       "      <td>27.4690</td>\n",
       "      <td>0.123134</td>\n",
       "      <td>3</td>\n",
       "      <td>ACN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.074182</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>0.042934</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.062329</td>\n",
       "      <td>0.043763</td>\n",
       "      <td>0.046747</td>\n",
       "      <td>0.077414</td>\n",
       "      <td>0.076668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>-0.0329</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>-0.0178</td>\n",
       "      <td>44.685183</td>\n",
       "      <td>0.843000</td>\n",
       "      <td>12.0650</td>\n",
       "      <td>0.144475</td>\n",
       "      <td>3</td>\n",
       "      <td>APOG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.148598</td>\n",
       "      <td>-0.148598</td>\n",
       "      <td>-0.056340</td>\n",
       "      <td>-0.056340</td>\n",
       "      <td>-0.104358</td>\n",
       "      <td>-0.104358</td>\n",
       "      <td>-0.096529</td>\n",
       "      <td>-0.096529</td>\n",
       "      <td>-0.048127</td>\n",
       "      <td>-0.048127</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>-0.0995</td>\n",
       "      <td>-0.0553</td>\n",
       "      <td>-0.0162</td>\n",
       "      <td>-2.099310</td>\n",
       "      <td>-0.256086</td>\n",
       "      <td>1.8264</td>\n",
       "      <td>-0.012180</td>\n",
       "      <td>2</td>\n",
       "      <td>BSET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>-0.002762</td>\n",
       "      <td>0.058128</td>\n",
       "      <td>0.093246</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.105302</td>\n",
       "      <td>0.038836</td>\n",
       "      <td>0.019425</td>\n",
       "      <td>0.011060</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0024</td>\n",
       "      <td>-0.0427</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>28.968078</td>\n",
       "      <td>0.759693</td>\n",
       "      <td>7.5317</td>\n",
       "      <td>0.423535</td>\n",
       "      <td>4</td>\n",
       "      <td>EBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05-29</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>-0.057143</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>-0.114286</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>-0.0085</td>\n",
       "      <td>1.093750</td>\n",
       "      <td>9.149215</td>\n",
       "      <td>1.4000</td>\n",
       "      <td>-0.202597</td>\n",
       "      <td>2</td>\n",
       "      <td>EMMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               day_1     day_2     day_3     day_4     day_5     day_6  \\\n",
       "date                                                                     \n",
       "2009-05-29 -0.074448 -0.032145 -0.034584 -0.060832 -0.066147 -0.055590   \n",
       "2009-05-29 -0.074182  0.005387  0.051057  0.042934  0.070700  0.062329   \n",
       "2009-05-29 -0.148598 -0.148598 -0.056340 -0.056340 -0.104358 -0.104358   \n",
       "2009-05-29 -0.002762  0.058128  0.093246  0.084071  0.105302  0.038836   \n",
       "2009-05-29  0.028571  0.028571 -0.057143 -0.057143 -0.114286 -0.114286   \n",
       "\n",
       "               day_7     day_8     day_9    day_10  ...  \\\n",
       "date                                                ...   \n",
       "2009-05-29 -0.081219 -0.063635 -0.052241 -0.030580  ...   \n",
       "2009-05-29  0.043763  0.046747  0.077414  0.076668  ...   \n",
       "2009-05-29 -0.096529 -0.096529 -0.048127 -0.048127  ...   \n",
       "2009-05-29  0.019425  0.011060  0.023155  0.044372  ...   \n",
       "2009-05-29 -0.028571 -0.028571  0.000000  0.000000  ...   \n",
       "\n",
       "            dividend_share_growth_10y  revenue_share_growth_3y  \\\n",
       "date                                                             \n",
       "2009-05-29                     0.0000                   0.0632   \n",
       "2009-05-29                     0.0450                  -0.0329   \n",
       "2009-05-29                    -1.0000                  -0.0995   \n",
       "2009-05-29                    -0.0024                  -0.0427   \n",
       "2009-05-29                     0.0000                  -0.1190   \n",
       "\n",
       "            revenue_share_growth_5y  revenue_share_growth_10y   pe_ratio  \\\n",
       "date                                                                       \n",
       "2009-05-29                   0.0637                    0.1173  40.395588   \n",
       "2009-05-29                   0.0296                   -0.0178  44.685183   \n",
       "2009-05-29                  -0.0553                   -0.0162  -2.099310   \n",
       "2009-05-29                   0.0081                    0.0634  28.968078   \n",
       "2009-05-29                   0.0137                   -0.0085   1.093750   \n",
       "\n",
       "            cash_share  norm_price    target  target_cat  symbol  \n",
       "date                                                              \n",
       "2009-05-29   37.358140     27.4690  0.123134           3     ACN  \n",
       "2009-05-29    0.843000     12.0650  0.144475           3    APOG  \n",
       "2009-05-29   -0.256086      1.8264 -0.012180           2    BSET  \n",
       "2009-05-29    0.759693      7.5317  0.423535           4     EBF  \n",
       "2009-05-29    9.149215      1.4000 -0.202597           2    EMMS  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create cache object\n",
    "cache = rcmd.stocks.Cache()\n",
    "\n",
    "# load list of all available stocks and sample sub-list\n",
    "stocks = cache.list_data('stock')\n",
    "\n",
    "def train_test_data(back, ahead, xlim, split=0.3, count=2000, stocks=stocks, cache=cache):\n",
    "    '''Generetes a train test split'''\n",
    "    sample = np.random.choice(list(stocks.keys()), 2000)\n",
    "    # split the stock data\n",
    "    count_train = int((1-split) * count)\n",
    "    sample_train = sample[:count_train]\n",
    "    sample_test = sample[count_train:]\n",
    "\n",
    "    # generate sample data\n",
    "    df_train = rcmd.learning.preprocess.create_dataset(sample_train, stocks, cache, back, ahead, xlim)\n",
    "    df_test = rcmd.learning.preprocess.create_dataset(sample_test, stocks, cache, back, ahead, xlim)\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "df_train, df_test = train_test_data(14, 66, (-.5, .5))\n",
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded and split the data, we have to divide it into input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_0      243\n",
       "cat_1     2047\n",
       "cat_2    15655\n",
       "cat_3    20405\n",
       "cat_4     3126\n",
       "cat_5     1066\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divide_data(df, xlim):\n",
    "    '''Splits the data into 3 sets: input, ouput_classify, output_regression.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DF to contain all relevant data\n",
    "        xlim (tuple): tuple of integers used to clip and scale regression values to a range of 0 to 1\n",
    "        \n",
    "    Returns:\n",
    "        df_X: DataFrame with input values\n",
    "        df_y_cls: DataFrame with classification labels\n",
    "        df_y_reg: DataFrame with regression values\n",
    "    '''\n",
    "    # remove all target cols\n",
    "    df_X = df.drop(['target', 'target_cat', 'norm_price', 'symbol'], axis=1)\n",
    "    # convert to dummy classes\n",
    "    df_y_cls = pd.get_dummies(df['target_cat'], prefix='cat', dummy_na=False)\n",
    "    # clip values and scale to vals\n",
    "    df_y_reg = np.divide( np.subtract( df['target'].clip(xlim[0], xlim[1]), xlim[0] ), (xlim[1] - xlim[0]) )\n",
    "    \n",
    "    return df_X, df_y_cls, df_y_reg\n",
    "\n",
    "X_train, y_ctrain, y_rtrain = divide_data(df_train, (-.5, .5))\n",
    "X_test, y_ctest, y_rtest = divide_data(df_test, (-.5, .5))\n",
    "y_ctrain.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create the actual prediction systems, we will have to define metrics, how we want to measure the success of the systems.\n",
    "As we have two approaches (classification and regression) we will use two types metrics:\n",
    "\n",
    "* Precision, Recall & Accuracy\n",
    "* RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric_classifier(y_true, y_pred, avg=None):\n",
    "    p = precision_score(y_true, y_pred, average=avg)\n",
    "    r = recall_score(y_true, y_pred, average=avg)\n",
    "    f1 = f1_score(y_true, y_pred, average=avg)\n",
    "    return f1, p, r\n",
    "\n",
    "def score_classifier(y_true, y_pred):\n",
    "    '''Calculates the relevant scores for a classifer and outputs. This should show predicitions per class.'''\n",
    "    f1, p, r = _metric_classifier(y_true, y_pred, avg='micro')\n",
    "    \n",
    "    print(\"Model Performance: F1={:.4f} (P={:.4f} / R={:.4f})\".format(f1, p, r))\n",
    "    # list scores of single classes\n",
    "    for i, c in enumerate(y_true.columns):\n",
    "        sf1, sp, sr = _metric_classifier(y_true.iloc[:, i], y_pred[:, i], avg='binary')\n",
    "        print(\"  {:10} F1={:.4f} (P={:.4f} / R={:.4f})\".format(c + \":\", sf1, sp, sr))\n",
    "        \n",
    "def score_regression(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    print(\"Model Performance: MSE={:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "The first step is to create a baseline for both approaches (classification and regression). In case of regression our target value will be `target` and for classification it will be `target_cat` (which we might convert into a one-hot vector along the way).\n",
    "\n",
    "Lets start with the simpler form of classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat_0      243\n",
       "cat_1     2047\n",
       "cat_2    15655\n",
       "cat_3    20405\n",
       "cat_4     3126\n",
       "cat_5     1066\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ctrain.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: F1=0.2655 (P=0.4987 / R=0.1809)\n",
      "  cat_0:     F1=0.0099 (P=0.0122 / R=0.0083)\n",
      "  cat_1:     F1=0.0138 (P=0.0654 / R=0.0077)\n",
      "  cat_2:     F1=0.0116 (P=0.5735 / R=0.0059)\n",
      "  cat_3:     F1=0.4302 (P=0.5175 / R=0.3682)\n",
      "  cat_4:     F1=0.0026 (P=0.0455 / R=0.0014)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    }
   ],
   "source": [
    "# scale input data to improve convergance (Note: scaler has to be used for other input data as well)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# train element\n",
    "classifier = MultiOutputClassifier(LogisticRegression(max_iter=500, solver='lbfgs'))\n",
    "classifier.fit(X_train_std, y_ctrain)\n",
    "\n",
    "# predict data\n",
    "y_pred = classifier.predict(X_test_std)\n",
    "\n",
    "score_classifier(y_ctest, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a strong bias in the system for `cat_3`, which also has the highest number of training samples. Future work might include oversampling or more target datapoint selection to reduce these biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, support vector machines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: F1=0.4157 (P=0.5416 / R=0.3372)\n",
      "  cat_0:     F1=0.0385 (P=1.0000 / R=0.0196)\n",
      "  cat_1:     F1=0.0154 (P=1.0000 / R=0.0078)\n",
      "  cat_2:     F1=0.0107 (P=0.4932 / R=0.0054)\n",
      "  cat_3:     F1=0.6126 (P=0.5414 / R=0.7053)\n",
      "  cat_4:     F1=0.0027 (P=1.0000 / R=0.0014)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "c:\\programming\\envs\\ds-stocks\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_svm = MultiOutputClassifier(SVC())\n",
    "classifier_svm.fit(X_train_std, y_ctrain)\n",
    "y_pred_svm = classifier_svm.predict(X_test_std)\n",
    "score_classifier(y_ctest, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42542, 31)\n",
      "Train on 42542 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C640DCE488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C640DCE488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.1531 - accuracy: 0.4854\n",
      "Epoch 2/100\n",
      "42542/42542 [==============================] - 3s 70us/sample - loss: 1.1172 - accuracy: 0.4941\n",
      "Epoch 3/100\n",
      "42542/42542 [==============================] - 3s 81us/sample - loss: 1.1088 - accuracy: 0.4949\n",
      "Epoch 4/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 1.1019 - accuracy: 0.4997\n",
      "Epoch 5/100\n",
      "42542/42542 [==============================] - 5s 111us/sample - loss: 1.0935 - accuracy: 0.5012\n",
      "Epoch 6/100\n",
      "42542/42542 [==============================] - 5s 114us/sample - loss: 1.0868 - accuracy: 0.5025\n",
      "Epoch 7/100\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.0799 - accuracy: 0.5063\n",
      "Epoch 8/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 1.0711 - accuracy: 0.5069\n",
      "Epoch 9/100\n",
      "42542/42542 [==============================] - 4s 84us/sample - loss: 1.0607 - accuracy: 0.5147\n",
      "Epoch 10/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 1.0489 - accuracy: 0.5195\n",
      "Epoch 11/100\n",
      "42542/42542 [==============================] - 4s 104us/sample - loss: 1.0353 - accuracy: 0.5243\n",
      "Epoch 12/100\n",
      "42542/42542 [==============================] - 4s 106us/sample - loss: 1.0225 - accuracy: 0.5308\n",
      "Epoch 13/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 1.0060 - accuracy: 0.5384\n",
      "Epoch 14/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.9885 - accuracy: 0.5468\n",
      "Epoch 15/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.9722 - accuracy: 0.5536\n",
      "Epoch 16/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.9523 - accuracy: 0.5638\n",
      "Epoch 17/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.9365 - accuracy: 0.5718\n",
      "Epoch 18/100\n",
      "42542/42542 [==============================] - 4s 85us/sample - loss: 0.9144 - accuracy: 0.5842\n",
      "Epoch 19/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.8935 - accuracy: 0.5948\n",
      "Epoch 20/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.8759 - accuracy: 0.6054\n",
      "Epoch 21/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.8558 - accuracy: 0.6148\n",
      "Epoch 22/100\n",
      "42542/42542 [==============================] - 4s 89us/sample - loss: 0.8323 - accuracy: 0.6250\n",
      "Epoch 23/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.8129 - accuracy: 0.6353\n",
      "Epoch 24/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7935 - accuracy: 0.6458\n",
      "Epoch 25/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.7729 - accuracy: 0.6553\n",
      "Epoch 26/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7562 - accuracy: 0.6633\n",
      "Epoch 27/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.7343 - accuracy: 0.6723\n",
      "Epoch 28/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.7226 - accuracy: 0.6781\n",
      "Epoch 29/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6981 - accuracy: 0.6896\n",
      "Epoch 30/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6826 - accuracy: 0.6996\n",
      "Epoch 31/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6660 - accuracy: 0.7085\n",
      "Epoch 32/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.6481 - accuracy: 0.7139\n",
      "Epoch 33/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6341 - accuracy: 0.7240\n",
      "Epoch 34/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6188 - accuracy: 0.7278\n",
      "Epoch 35/100\n",
      "42542/42542 [==============================] - 4s 88us/sample - loss: 0.6099 - accuracy: 0.7349\n",
      "Epoch 36/100\n",
      "42542/42542 [==============================] - 4s 87us/sample - loss: 0.5840 - accuracy: 0.7457\n",
      "Epoch 37/100\n",
      "42542/42542 [==============================] - 6s 143us/sample - loss: 0.5736 - accuracy: 0.7502\n",
      "Epoch 38/100\n",
      "42542/42542 [==============================] - 6s 141us/sample - loss: 0.5612 - accuracy: 0.7564\n",
      "Epoch 39/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.5490 - accuracy: 0.7634\n",
      "Epoch 40/100\n",
      "42542/42542 [==============================] - 5s 119us/sample - loss: 0.5330 - accuracy: 0.7714\n",
      "Epoch 41/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.5225 - accuracy: 0.7764\n",
      "Epoch 42/100\n",
      "42542/42542 [==============================] - 5s 113us/sample - loss: 0.5121 - accuracy: 0.7831\n",
      "Epoch 43/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.4988 - accuracy: 0.7867\n",
      "Epoch 44/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.4900 - accuracy: 0.7931\n",
      "Epoch 45/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.4762 - accuracy: 0.7978\n",
      "Epoch 46/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.4740 - accuracy: 0.7989\n",
      "Epoch 47/100\n",
      "42542/42542 [==============================] - 4s 91us/sample - loss: 0.4571 - accuracy: 0.8063\n",
      "Epoch 48/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.4534 - accuracy: 0.8068\n",
      "Epoch 49/100\n",
      "42542/42542 [==============================] - 4s 90us/sample - loss: 0.4449 - accuracy: 0.8121\n",
      "Epoch 50/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.4258 - accuracy: 0.8225\n",
      "Epoch 51/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.4384 - accuracy: 0.8166\n",
      "Epoch 52/100\n",
      "42542/42542 [==============================] - 4s 91us/sample - loss: 0.4131 - accuracy: 0.8269\n",
      "Epoch 53/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.4055 - accuracy: 0.8301\n",
      "Epoch 54/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.4062 - accuracy: 0.8316\n",
      "Epoch 55/100\n",
      "42542/42542 [==============================] - 4s 102us/sample - loss: 0.3876 - accuracy: 0.8373\n",
      "Epoch 56/100\n",
      "42542/42542 [==============================] - 4s 89us/sample - loss: 0.3972 - accuracy: 0.8367\n",
      "Epoch 57/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3819 - accuracy: 0.8411\n",
      "Epoch 58/100\n",
      "42542/42542 [==============================] - 4s 92us/sample - loss: 0.3795 - accuracy: 0.8420\n",
      "Epoch 59/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3719 - accuracy: 0.8467\n",
      "Epoch 60/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.3646 - accuracy: 0.8498\n",
      "Epoch 61/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3613 - accuracy: 0.8508\n",
      "Epoch 62/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3662 - accuracy: 0.8507\n",
      "Epoch 63/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.3438 - accuracy: 0.8568\n",
      "Epoch 64/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.3375 - accuracy: 0.8624\n",
      "Epoch 65/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.3443 - accuracy: 0.8601\n",
      "Epoch 66/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.3440 - accuracy: 0.8614\n",
      "Epoch 67/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.3274 - accuracy: 0.8674\n",
      "Epoch 68/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.3149 - accuracy: 0.8709\n",
      "Epoch 69/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.3205 - accuracy: 0.8671\n",
      "Epoch 70/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3212 - accuracy: 0.8711\n",
      "Epoch 71/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3238 - accuracy: 0.8700\n",
      "Epoch 72/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2898 - accuracy: 0.8831\n",
      "Epoch 73/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.3070 - accuracy: 0.8757\n",
      "Epoch 74/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.3022 - accuracy: 0.8788\n",
      "Epoch 75/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.2943 - accuracy: 0.8834\n",
      "Epoch 76/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2929 - accuracy: 0.8831\n",
      "Epoch 77/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2907 - accuracy: 0.8831\n",
      "Epoch 78/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2920 - accuracy: 0.8823\n",
      "Epoch 79/100\n",
      "42542/42542 [==============================] - 4s 95us/sample - loss: 0.2871 - accuracy: 0.8837\n",
      "Epoch 80/100\n",
      "42542/42542 [==============================] - 5s 127us/sample - loss: 0.2744 - accuracy: 0.8897\n",
      "Epoch 81/100\n",
      "42542/42542 [==============================] - 6s 134us/sample - loss: 0.2799 - accuracy: 0.8883\n",
      "Epoch 82/100\n",
      "42542/42542 [==============================] - 7s 154us/sample - loss: 0.2748 - accuracy: 0.8916\n",
      "Epoch 83/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2737 - accuracy: 0.8919\n",
      "Epoch 84/100\n",
      "42542/42542 [==============================] - 4s 96us/sample - loss: 0.2634 - accuracy: 0.8975\n",
      "Epoch 85/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2730 - accuracy: 0.8910\n",
      "Epoch 86/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2512 - accuracy: 0.9009\n",
      "Epoch 87/100\n",
      "42542/42542 [==============================] - 5s 109us/sample - loss: 0.2773 - accuracy: 0.8934\n",
      "Epoch 88/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2676 - accuracy: 0.8977\n",
      "Epoch 89/100\n",
      "42542/42542 [==============================] - 5s 116us/sample - loss: 0.2532 - accuracy: 0.9015\n",
      "Epoch 90/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.2509 - accuracy: 0.9010\n",
      "Epoch 91/100\n",
      "42542/42542 [==============================] - 4s 102us/sample - loss: 0.2430 - accuracy: 0.9045\n",
      "Epoch 92/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2454 - accuracy: 0.9057\n",
      "Epoch 93/100\n",
      "42542/42542 [==============================] - 4s 98us/sample - loss: 0.2619 - accuracy: 0.9009\n",
      "Epoch 94/100\n",
      "42542/42542 [==============================] - 4s 93us/sample - loss: 0.2472 - accuracy: 0.9070\n",
      "Epoch 95/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2335 - accuracy: 0.9083\n",
      "Epoch 96/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2351 - accuracy: 0.9111\n",
      "Epoch 97/100\n",
      "42542/42542 [==============================] - 4s 99us/sample - loss: 0.2414 - accuracy: 0.9064\n",
      "Epoch 98/100\n",
      "42542/42542 [==============================] - 4s 97us/sample - loss: 0.2448 - accuracy: 0.9054\n",
      "Epoch 99/100\n",
      "42542/42542 [==============================] - 4s 94us/sample - loss: 0.2324 - accuracy: 0.9098\n",
      "Epoch 100/100\n",
      "42542/42542 [==============================] - 4s 100us/sample - loss: 0.2190 - accuracy: 0.9149\n",
      "Model Performance: F1=0.4841 (P=0.4841 / R=0.4841)\n",
      "  cat_0:     F1=0.1386 (P=0.1707 / R=0.1167)\n",
      "  cat_1:     F1=0.2188 (P=0.2192 / R=0.2184)\n",
      "  cat_2:     F1=0.4441 (P=0.4597 / R=0.4296)\n",
      "  cat_3:     F1=0.5946 (P=0.5822 / R=0.6076)\n",
      "  cat_4:     F1=0.2565 (P=0.2559 / R=0.2571)\n",
      "  cat_5:     F1=0.1798 (P=0.1670 / R=0.1947)\n"
     ]
    }
   ],
   "source": [
    "# simple feed forward network\n",
    "print(X_train.shape)\n",
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train_std, df_train['target_cat'].to_numpy(), epochs=100)\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test_std)\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that the output of the model in the test data resembles the input distribution. Lets try to improve generalization with a more complex model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42542 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C64CFE2488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C64CFE2488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 10s 247us/sample - loss: 1.1949 - accuracy: 0.4635\n",
      "Epoch 2/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.1519 - accuracy: 0.4784\n",
      "Epoch 3/200\n",
      "42542/42542 [==============================] - 9s 221us/sample - loss: 1.1448 - accuracy: 0.4842\n",
      "Epoch 4/200\n",
      "42542/42542 [==============================] - 11s 260us/sample - loss: 1.1401 - accuracy: 0.4881\n",
      "Epoch 5/200\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.1386 - accuracy: 0.4886\n",
      "Epoch 6/200\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.1358 - accuracy: 0.4889\n",
      "Epoch 7/200\n",
      "42542/42542 [==============================] - 10s 236us/sample - loss: 1.1335 - accuracy: 0.4916\n",
      "Epoch 8/200\n",
      "42542/42542 [==============================] - 10s 241us/sample - loss: 1.1304 - accuracy: 0.4888\n",
      "Epoch 9/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.1283 - accuracy: 0.4912\n",
      "Epoch 10/200\n",
      "42542/42542 [==============================] - 9s 223us/sample - loss: 1.1255 - accuracy: 0.4902\n",
      "Epoch 11/200\n",
      "42542/42542 [==============================] - 9s 208us/sample - loss: 1.1225 - accuracy: 0.4923\n",
      "Epoch 12/200\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.1223 - accuracy: 0.4914\n",
      "Epoch 13/200\n",
      "42542/42542 [==============================] - 8s 197us/sample - loss: 1.1193 - accuracy: 0.4921\n",
      "Epoch 14/200\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.1181 - accuracy: 0.4909\n",
      "Epoch 15/200\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.1163 - accuracy: 0.4926\n",
      "Epoch 16/200\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.1159 - accuracy: 0.4938\n",
      "Epoch 17/200\n",
      "42542/42542 [==============================] - 12s 286us/sample - loss: 1.1152 - accuracy: 0.4925\n",
      "Epoch 18/200\n",
      "42542/42542 [==============================] - 9s 205us/sample - loss: 1.1143 - accuracy: 0.4916\n",
      "Epoch 19/200\n",
      "42542/42542 [==============================] - 10s 226us/sample - loss: 1.1133 - accuracy: 0.4913\n",
      "Epoch 20/200\n",
      "42542/42542 [==============================] - 10s 236us/sample - loss: 1.1131 - accuracy: 0.4932\n",
      "Epoch 21/200\n",
      "42542/42542 [==============================] - 11s 258us/sample - loss: 1.1121 - accuracy: 0.4930\n",
      "Epoch 22/200\n",
      "42542/42542 [==============================] - 10s 246us/sample - loss: 1.1114 - accuracy: 0.4939\n",
      "Epoch 23/200\n",
      "42542/42542 [==============================] - 9s 205us/sample - loss: 1.1093 - accuracy: 0.4938\n",
      "Epoch 24/200\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.1100 - accuracy: 0.4955\n",
      "Epoch 25/200\n",
      "42542/42542 [==============================] - 8s 199us/sample - loss: 1.1085 - accuracy: 0.4941\n",
      "Epoch 26/200\n",
      "42542/42542 [==============================] - 9s 204us/sample - loss: 1.1079 - accuracy: 0.4937\n",
      "Epoch 27/200\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.1077 - accuracy: 0.4955\n",
      "Epoch 28/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.1059 - accuracy: 0.4946\n",
      "Epoch 29/200\n",
      "42542/42542 [==============================] - 16s 370us/sample - loss: 1.1051 - accuracy: 0.4954\n",
      "Epoch 30/200\n",
      "42542/42542 [==============================] - 11s 268us/sample - loss: 1.1055 - accuracy: 0.4964\n",
      "Epoch 31/200\n",
      "42542/42542 [==============================] - 11s 260us/sample - loss: 1.1047 - accuracy: 0.4951\n",
      "Epoch 32/200\n",
      "42542/42542 [==============================] - 9s 220us/sample - loss: 1.1028 - accuracy: 0.4945\n",
      "Epoch 33/200\n",
      "42542/42542 [==============================] - 10s 236us/sample - loss: 1.1016 - accuracy: 0.4969\n",
      "Epoch 34/200\n",
      "42542/42542 [==============================] - 11s 250us/sample - loss: 1.1039 - accuracy: 0.4929\n",
      "Epoch 35/200\n",
      "42542/42542 [==============================] - 11s 249us/sample - loss: 1.1020 - accuracy: 0.4961\n",
      "Epoch 36/200\n",
      "42542/42542 [==============================] - 11s 259us/sample - loss: 1.1002 - accuracy: 0.4944\n",
      "Epoch 37/200\n",
      "42542/42542 [==============================] - 13s 303us/sample - loss: 1.0988 - accuracy: 0.4961\n",
      "Epoch 38/200\n",
      "42542/42542 [==============================] - 14s 332us/sample - loss: 1.0993 - accuracy: 0.4980- loss: 1.0994 \n",
      "Epoch 39/200\n",
      "42542/42542 [==============================] - 14s 333us/sample - loss: 1.0984 - accuracy: 0.4968\n",
      "Epoch 40/200\n",
      "42542/42542 [==============================] - 11s 269us/sample - loss: 1.0983 - accuracy: 0.4963\n",
      "Epoch 41/200\n",
      "42542/42542 [==============================] - 11s 269us/sample - loss: 1.0976 - accuracy: 0.4961\n",
      "Epoch 42/200\n",
      "42542/42542 [==============================] - 10s 228us/sample - loss: 1.0969 - accuracy: 0.4961\n",
      "Epoch 43/200\n",
      "42542/42542 [==============================] - 10s 230us/sample - loss: 1.0955 - accuracy: 0.4969\n",
      "Epoch 44/200\n",
      "42542/42542 [==============================] - 10s 231us/sample - loss: 1.0958 - accuracy: 0.4965\n",
      "Epoch 45/200\n",
      "42542/42542 [==============================] - 10s 230us/sample - loss: 1.0963 - accuracy: 0.4968\n",
      "Epoch 46/200\n",
      "42542/42542 [==============================] - 12s 291us/sample - loss: 1.0945 - accuracy: 0.4985- los\n",
      "Epoch 47/200\n",
      "42542/42542 [==============================] - 13s 307us/sample - loss: 1.0942 - accuracy: 0.4973\n",
      "Epoch 48/200\n",
      "42542/42542 [==============================] - 10s 231us/sample - loss: 1.0923 - accuracy: 0.4985\n",
      "Epoch 49/200\n",
      "42542/42542 [==============================] - 10s 226us/sample - loss: 1.0909 - accuracy: 0.4991\n",
      "Epoch 50/200\n",
      "42542/42542 [==============================] - 10s 234us/sample - loss: 1.0923 - accuracy: 0.5000\n",
      "Epoch 51/200\n",
      "42542/42542 [==============================] - 10s 239us/sample - loss: 1.0917 - accuracy: 0.4984\n",
      "Epoch 52/200\n",
      "42542/42542 [==============================] - 10s 226us/sample - loss: 1.0906 - accuracy: 0.4994\n",
      "Epoch 53/200\n",
      "42542/42542 [==============================] - 11s 249us/sample - loss: 1.0891 - accuracy: 0.5004\n",
      "Epoch 54/200\n",
      "42542/42542 [==============================] - 10s 244us/sample - loss: 1.0882 - accuracy: 0.4999\n",
      "Epoch 55/200\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.0877 - accuracy: 0.4998\n",
      "Epoch 56/200\n",
      "42542/42542 [==============================] - 10s 229us/sample - loss: 1.0871 - accuracy: 0.5018\n",
      "Epoch 57/200\n",
      "42542/42542 [==============================] - 10s 224us/sample - loss: 1.0879 - accuracy: 0.4986\n",
      "Epoch 58/200\n",
      "42542/42542 [==============================] - 11s 253us/sample - loss: 1.0870 - accuracy: 0.4986\n",
      "Epoch 59/200\n",
      "42542/42542 [==============================] - 11s 252us/sample - loss: 1.0852 - accuracy: 0.5005\n",
      "Epoch 60/200\n",
      "42542/42542 [==============================] - 10s 229us/sample - loss: 1.0854 - accuracy: 0.5000\n",
      "Epoch 61/200\n",
      "42542/42542 [==============================] - 10s 231us/sample - loss: 1.0864 - accuracy: 0.4984\n",
      "Epoch 62/200\n",
      "42542/42542 [==============================] - 10s 236us/sample - loss: 1.0844 - accuracy: 0.5008\n",
      "Epoch 63/200\n",
      "42542/42542 [==============================] - 10s 231us/sample - loss: 1.0836 - accuracy: 0.5012\n",
      "Epoch 64/200\n",
      "42542/42542 [==============================] - 10s 245us/sample - loss: 1.0831 - accuracy: 0.4999\n",
      "Epoch 65/200\n",
      "42542/42542 [==============================] - 11s 254us/sample - loss: 1.0822 - accuracy: 0.5020\n",
      "Epoch 66/200\n",
      "42542/42542 [==============================] - 10s 232us/sample - loss: 1.0815 - accuracy: 0.4998\n",
      "Epoch 67/200\n",
      "42542/42542 [==============================] - 10s 235us/sample - loss: 1.0805 - accuracy: 0.5032\n",
      "Epoch 68/200\n",
      "42542/42542 [==============================] - 10s 229us/sample - loss: 1.0824 - accuracy: 0.5009\n",
      "Epoch 69/200\n",
      "42542/42542 [==============================] - 10s 225us/sample - loss: 1.0795 - accuracy: 0.5034\n",
      "Epoch 70/200\n",
      "42542/42542 [==============================] - 12s 280us/sample - loss: 1.0789 - accuracy: 0.5031\n",
      "Epoch 71/200\n",
      "42542/42542 [==============================] - 9s 220us/sample - loss: 1.0797 - accuracy: 0.5034\n",
      "Epoch 72/200\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0778 - accuracy: 0.5032\n",
      "Epoch 73/200\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0781 - accuracy: 0.5042\n",
      "Epoch 74/200\n",
      "42542/42542 [==============================] - 8s 197us/sample - loss: 1.0767 - accuracy: 0.5058\n",
      "Epoch 75/200\n",
      "42542/42542 [==============================] - 8s 198us/sample - loss: 1.0770 - accuracy: 0.5030\n",
      "Epoch 76/200\n",
      "42542/42542 [==============================] - 8s 198us/sample - loss: 1.0776 - accuracy: 0.5030\n",
      "Epoch 77/200\n",
      "42542/42542 [==============================] - 9s 219us/sample - loss: 1.0741 - accuracy: 0.5048\n",
      "Epoch 78/200\n",
      "42542/42542 [==============================] - 10s 238us/sample - loss: 1.0734 - accuracy: 0.5047\n",
      "Epoch 79/200\n",
      "42542/42542 [==============================] - 12s 293us/sample - loss: 1.0738 - accuracy: 0.5056\n",
      "Epoch 80/200\n",
      "42542/42542 [==============================] - 10s 242us/sample - loss: 1.0752 - accuracy: 0.5060\n",
      "Epoch 81/200\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.0715 - accuracy: 0.5056\n",
      "Epoch 82/200\n",
      "42542/42542 [==============================] - 12s 285us/sample - loss: 1.0711 - accuracy: 0.5043\n",
      "Epoch 83/200\n",
      "42542/42542 [==============================] - 10s 232us/sample - loss: 1.0726 - accuracy: 0.5067\n",
      "Epoch 84/200\n",
      "42542/42542 [==============================] - 9s 214us/sample - loss: 1.0705 - accuracy: 0.5052\n",
      "Epoch 85/200\n",
      "42542/42542 [==============================] - 10s 228us/sample - loss: 1.0720 - accuracy: 0.5066\n",
      "Epoch 86/200\n",
      "42542/42542 [==============================] - 9s 221us/sample - loss: 1.0706 - accuracy: 0.5064\n",
      "Epoch 87/200\n",
      "42542/42542 [==============================] - 9s 207us/sample - loss: 1.0675 - accuracy: 0.5065\n",
      "Epoch 88/200\n",
      "42542/42542 [==============================] - 9s 223us/sample - loss: 1.0674 - accuracy: 0.5092\n",
      "Epoch 89/200\n",
      "42542/42542 [==============================] - 11s 252us/sample - loss: 1.0682 - accuracy: 0.5077\n",
      "Epoch 90/200\n",
      "42542/42542 [==============================] - 10s 245us/sample - loss: 1.0682 - accuracy: 0.5056\n",
      "Epoch 91/200\n",
      "42542/42542 [==============================] - 11s 261us/sample - loss: 1.0681 - accuracy: 0.5070\n",
      "Epoch 92/200\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.0668 - accuracy: 0.5080\n",
      "Epoch 93/200\n",
      "42542/42542 [==============================] - 11s 269us/sample - loss: 1.0642 - accuracy: 0.5111\n",
      "Epoch 94/200\n",
      "42542/42542 [==============================] - 11s 267us/sample - loss: 1.0650 - accuracy: 0.5112\n",
      "Epoch 95/200\n",
      "42542/42542 [==============================] - 11s 259us/sample - loss: 1.0648 - accuracy: 0.5079\n",
      "Epoch 96/200\n",
      "42542/42542 [==============================] - 13s 314us/sample - loss: 1.0645 - accuracy: 0.5085\n",
      "Epoch 97/200\n",
      "42542/42542 [==============================] - 12s 272us/sample - loss: 1.0631 - accuracy: 0.5079\n",
      "Epoch 98/200\n",
      "42542/42542 [==============================] - 11s 248us/sample - loss: 1.0620 - accuracy: 0.5097\n",
      "Epoch 99/200\n",
      "42542/42542 [==============================] - 11s 264us/sample - loss: 1.0632 - accuracy: 0.5081\n",
      "Epoch 100/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0613 - accuracy: 0.5120\n",
      "Epoch 101/200\n",
      "42542/42542 [==============================] - 8s 198us/sample - loss: 1.0620 - accuracy: 0.5109\n",
      "Epoch 102/200\n",
      "42542/42542 [==============================] - 10s 229us/sample - loss: 1.0673 - accuracy: 0.5077\n",
      "Epoch 103/200\n",
      "42542/42542 [==============================] - 10s 224us/sample - loss: 1.0628 - accuracy: 0.5103- loss: 1.0\n",
      "Epoch 104/200\n",
      "42542/42542 [==============================] - 10s 232us/sample - loss: 1.0584 - accuracy: 0.5110\n",
      "Epoch 105/200\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.0578 - accuracy: 0.5128\n",
      "Epoch 106/200\n",
      "42542/42542 [==============================] - 12s 285us/sample - loss: 1.0600 - accuracy: 0.5115\n",
      "Epoch 107/200\n",
      "42542/42542 [==============================] - 11s 259us/sample - loss: 1.0654 - accuracy: 0.5089\n",
      "Epoch 108/200\n",
      "42542/42542 [==============================] - 13s 315us/sample - loss: 1.0573 - accuracy: 0.5118\n",
      "Epoch 109/200\n",
      "42542/42542 [==============================] - 16s 378us/sample - loss: 1.0559 - accuracy: 0.5148\n",
      "Epoch 110/200\n",
      "42542/42542 [==============================] - 13s 302us/sample - loss: 1.0571 - accuracy: 0.5127\n",
      "Epoch 111/200\n",
      "42542/42542 [==============================] - 18s 414us/sample - loss: 1.0594 - accuracy: 0.5119\n",
      "Epoch 112/200\n",
      "42542/42542 [==============================] - 15s 346us/sample - loss: 1.0541 - accuracy: 0.5119\n",
      "Epoch 113/200\n",
      "42542/42542 [==============================] - 15s 353us/sample - loss: 1.0561 - accuracy: 0.5114\n",
      "Epoch 114/200\n",
      "42542/42542 [==============================] - 13s 310us/sample - loss: 1.0547 - accuracy: 0.5142\n",
      "Epoch 115/200\n",
      "42542/42542 [==============================] - 10s 224us/sample - loss: 1.0536 - accuracy: 0.5147\n",
      "Epoch 116/200\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.0545 - accuracy: 0.5138\n",
      "Epoch 117/200\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.0538 - accuracy: 0.5138\n",
      "Epoch 118/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0528 - accuracy: 0.5150\n",
      "Epoch 119/200\n",
      "42542/42542 [==============================] - 8s 197us/sample - loss: 1.0530 - accuracy: 0.5123\n",
      "Epoch 120/200\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.0594 - accuracy: 0.5109\n",
      "Epoch 121/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0543 - accuracy: 0.5126\n",
      "Epoch 122/200\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0525 - accuracy: 0.5124\n",
      "Epoch 123/200\n",
      "42542/42542 [==============================] - 9s 201us/sample - loss: 1.0504 - accuracy: 0.5152\n",
      "Epoch 124/200\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.0512 - accuracy: 0.5118\n",
      "Epoch 125/200\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0508 - accuracy: 0.5158\n",
      "Epoch 126/200\n",
      "42542/42542 [==============================] - 8s 197us/sample - loss: 1.0516 - accuracy: 0.5156\n",
      "Epoch 127/200\n",
      "42542/42542 [==============================] - 8s 191us/sample - loss: 1.0537 - accuracy: 0.5141\n",
      "Epoch 128/200\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0515 - accuracy: 0.5156\n",
      "Epoch 129/200\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0491 - accuracy: 0.5144\n",
      "Epoch 130/200\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0483 - accuracy: 0.5202\n",
      "Epoch 131/200\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0461 - accuracy: 0.5164\n",
      "Epoch 132/200\n",
      "42542/42542 [==============================] - 8s 198us/sample - loss: 1.0465 - accuracy: 0.5180\n",
      "Epoch 133/200\n",
      "42542/42542 [==============================] - 10s 246us/sample - loss: 1.0454 - accuracy: 0.5178\n",
      "Epoch 134/200\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0441 - accuracy: 0.5186\n",
      "Epoch 135/200\n",
      "42542/42542 [==============================] - 9s 218us/sample - loss: 1.0437 - accuracy: 0.5189\n",
      "Epoch 136/200\n",
      "42542/42542 [==============================] - 9s 204us/sample - loss: 1.0430 - accuracy: 0.5182\n",
      "Epoch 137/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0506 - accuracy: 0.5169\n",
      "Epoch 138/200\n",
      "42542/42542 [==============================] - 8s 194us/sample - loss: 1.0504 - accuracy: 0.5141\n",
      "Epoch 139/200\n",
      "42542/42542 [==============================] - 8s 196us/sample - loss: 1.0470 - accuracy: 0.5165\n",
      "Epoch 140/200\n",
      "42542/42542 [==============================] - 8s 199us/sample - loss: 1.0456 - accuracy: 0.5166\n",
      "Epoch 141/200\n",
      "42542/42542 [==============================] - 11s 262us/sample - loss: 1.0415 - accuracy: 0.5170\n",
      "Epoch 142/200\n",
      "42542/42542 [==============================] - 10s 236us/sample - loss: 1.0430 - accuracy: 0.5186\n",
      "Epoch 143/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0434 - accuracy: 0.5196\n",
      "Epoch 144/200\n",
      "42542/42542 [==============================] - 8s 192us/sample - loss: 1.0412 - accuracy: 0.5198\n",
      "Epoch 145/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0424 - accuracy: 0.5178\n",
      "Epoch 146/200\n",
      "42542/42542 [==============================] - 8s 195us/sample - loss: 1.0425 - accuracy: 0.5182\n",
      "Epoch 147/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0415 - accuracy: 0.5197\n",
      "Epoch 148/200\n",
      "42542/42542 [==============================] - 8s 193us/sample - loss: 1.0396 - accuracy: 0.5203\n",
      "Epoch 149/200\n",
      "42542/42542 [==============================] - 10s 229us/sample - loss: 1.0396 - accuracy: 0.5202\n",
      "Epoch 150/200\n",
      "42542/42542 [==============================] - 10s 230us/sample - loss: 1.0365 - accuracy: 0.5186\n",
      "Epoch 151/200\n",
      "42542/42542 [==============================] - 11s 249us/sample - loss: 1.0412 - accuracy: 0.5198\n",
      "Epoch 152/200\n",
      "42542/42542 [==============================] - 11s 265us/sample - loss: 1.0370 - accuracy: 0.5213\n",
      "Epoch 153/200\n",
      "42542/42542 [==============================] - 11s 266us/sample - loss: 1.0370 - accuracy: 0.5210\n",
      "Epoch 154/200\n",
      "42542/42542 [==============================] - 9s 204us/sample - loss: 1.0350 - accuracy: 0.5224\n",
      "Epoch 155/200\n",
      "42542/42542 [==============================] - 11s 254us/sample - loss: 1.0365 - accuracy: 0.5222\n",
      "Epoch 156/200\n",
      "42542/42542 [==============================] - 13s 296us/sample - loss: 1.0354 - accuracy: 0.5227\n",
      "Epoch 157/200\n",
      "42542/42542 [==============================] - 12s 284us/sample - loss: 1.0355 - accuracy: 0.5205\n",
      "Epoch 158/200\n",
      "42542/42542 [==============================] - 10s 233us/sample - loss: 1.0391 - accuracy: 0.5206\n",
      "Epoch 159/200\n",
      "42542/42542 [==============================] - 12s 281us/sample - loss: 1.0338 - accuracy: 0.5224\n",
      "Epoch 160/200\n",
      "42542/42542 [==============================] - 10s 228us/sample - loss: 1.0337 - accuracy: 0.5218\n",
      "Epoch 161/200\n",
      "42542/42542 [==============================] - 10s 247us/sample - loss: 1.0341 - accuracy: 0.5232\n",
      "Epoch 162/200\n",
      "42542/42542 [==============================] - 9s 222us/sample - loss: 1.0356 - accuracy: 0.5194\n",
      "Epoch 163/200\n",
      "42542/42542 [==============================] - 9s 219us/sample - loss: 1.0343 - accuracy: 0.5199\n",
      "Epoch 164/200\n",
      "42542/42542 [==============================] - 9s 220us/sample - loss: 1.0330 - accuracy: 0.5235\n",
      "Epoch 165/200\n",
      "42542/42542 [==============================] - 11s 259us/sample - loss: 1.0310 - accuracy: 0.5232\n",
      "Epoch 166/200\n",
      "42542/42542 [==============================] - 10s 240us/sample - loss: 1.0328 - accuracy: 0.5238\n",
      "Epoch 167/200\n",
      "42542/42542 [==============================] - 12s 274us/sample - loss: 1.0328 - accuracy: 0.5244\n",
      "Epoch 168/200\n",
      "42542/42542 [==============================] - 12s 282us/sample - loss: 1.0298 - accuracy: 0.5236\n",
      "Epoch 169/200\n",
      "42542/42542 [==============================] - 10s 239us/sample - loss: 1.0286 - accuracy: 0.5258\n",
      "Epoch 170/200\n",
      "42542/42542 [==============================] - 12s 282us/sample - loss: 1.0304 - accuracy: 0.5252\n",
      "Epoch 171/200\n",
      "42542/42542 [==============================] - 11s 261us/sample - loss: 1.0321 - accuracy: 0.5243\n",
      "Epoch 172/200\n",
      "42542/42542 [==============================] - 15s 351us/sample - loss: 1.0280 - accuracy: 0.5255\n",
      "Epoch 173/200\n",
      "42542/42542 [==============================] - 14s 328us/sample - loss: 1.0299 - accuracy: 0.5264\n",
      "Epoch 174/200\n",
      "42542/42542 [==============================] - 12s 281us/sample - loss: 1.0274 - accuracy: 0.5239\n",
      "Epoch 175/200\n",
      "42542/42542 [==============================] - 10s 234us/sample - loss: 1.0258 - accuracy: 0.5272\n",
      "Epoch 176/200\n",
      "42542/42542 [==============================] - 9s 208us/sample - loss: 1.0242 - accuracy: 0.5262\n",
      "Epoch 177/200\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0261 - accuracy: 0.5255\n",
      "Epoch 178/200\n",
      "42542/42542 [==============================] - 9s 223us/sample - loss: 1.0323 - accuracy: 0.5223\n",
      "Epoch 179/200\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0240 - accuracy: 0.5252\n",
      "Epoch 180/200\n",
      "42542/42542 [==============================] - 9s 207us/sample - loss: 1.0256 - accuracy: 0.5255\n",
      "Epoch 181/200\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0229 - accuracy: 0.5270\n",
      "Epoch 182/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0205 - accuracy: 0.5294\n",
      "Epoch 183/200\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0246 - accuracy: 0.5239\n",
      "Epoch 184/200\n",
      "42542/42542 [==============================] - 10s 235us/sample - loss: 1.0217 - accuracy: 0.5261\n",
      "Epoch 185/200\n",
      "42542/42542 [==============================] - 10s 230us/sample - loss: 1.0227 - accuracy: 0.5290\n",
      "Epoch 186/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0226 - accuracy: 0.5263\n",
      "Epoch 187/200\n",
      "42542/42542 [==============================] - 9s 219us/sample - loss: 1.0222 - accuracy: 0.5264\n",
      "Epoch 188/200\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0200 - accuracy: 0.5293\n",
      "Epoch 189/200\n",
      "42542/42542 [==============================] - 9s 212us/sample - loss: 1.0201 - accuracy: 0.5297\n",
      "Epoch 190/200\n",
      "42542/42542 [==============================] - 9s 213us/sample - loss: 1.0200 - accuracy: 0.5292\n",
      "Epoch 191/200\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.0201 - accuracy: 0.5271\n",
      "Epoch 192/200\n",
      "42542/42542 [==============================] - 9s 205us/sample - loss: 1.0193 - accuracy: 0.5288\n",
      "Epoch 193/200\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.0186 - accuracy: 0.5287\n",
      "Epoch 194/200\n",
      "42542/42542 [==============================] - 9s 209us/sample - loss: 1.0176 - accuracy: 0.5290\n",
      "Epoch 195/200\n",
      "42542/42542 [==============================] - 9s 216us/sample - loss: 1.0181 - accuracy: 0.5286\n",
      "Epoch 196/200\n",
      "42542/42542 [==============================] - 9s 210us/sample - loss: 1.0192 - accuracy: 0.5294\n",
      "Epoch 197/200\n",
      "42542/42542 [==============================] - 11s 248us/sample - loss: 1.0183 - accuracy: 0.5293\n",
      "Epoch 198/200\n",
      "42542/42542 [==============================] - 9s 203us/sample - loss: 1.0182 - accuracy: 0.5305\n",
      "Epoch 199/200\n",
      "42542/42542 [==============================] - 9s 215us/sample - loss: 1.0235 - accuracy: 0.5280\n",
      "Epoch 200/200\n",
      "42542/42542 [==============================] - 9s 202us/sample - loss: 1.0157 - accuracy: 0.5307\n",
      "0        4\n",
      "1       38\n",
      "2      468\n",
      "3    18096\n",
      "4        1\n",
      "5       15\n",
      "dtype: int64\n",
      "Model Performance: F1=0.4850 (P=0.4850 / R=0.4850)\n",
      "  cat_0:     F1=0.0161 (P=0.2500 / R=0.0083)\n",
      "  cat_1:     F1=0.0084 (P=0.1053 / R=0.0044)\n",
      "  cat_2:     F1=0.0579 (P=0.4402 / R=0.0310)\n",
      "  cat_3:     F1=0.6507 (P=0.4875 / R=0.9783)\n",
      "  cat_4:     F1=0.0000 (P=0.0000 / R=0.0000)\n",
      "  cat_5:     F1=0.0000 (P=0.0000 / R=0.0000)\n"
     ]
    }
   ],
   "source": [
    "act = tf.keras.layers.PReLU\n",
    "classifier_ffn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(32), act(),\n",
    "    tf.keras.layers.Dense(64), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128), act(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(256), act(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(128), act(),\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(64), act(),\n",
    "    tf.keras.layers.Dense(y_ctrain.shape[1], activation=tf.nn.softmax)\n",
    "])\n",
    "classifier_ffn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier_ffn.fit(X_train.to_numpy(), df_train['target_cat'].to_numpy(), epochs=200)\n",
    "\n",
    "y_pred_ffn = classifier_ffn.predict(X_test.to_numpy())\n",
    "print(y_pred_ffn)\n",
    "y_pred_ffn = pd.get_dummies(y_pred_ffn.argmax(axis=1))\n",
    "print(y_pred_ffn.sum(axis=0))\n",
    "score_classifier(y_ctest, y_pred_ffn.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "classifier_ffn.save('../data/keras-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "The other possible option is regression. We will test a linear regression against neural networks based on RMSE score to see how the predictions hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: MSE=0.0329\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train.iloc[:, :7].to_numpy(), y_rtrain)\n",
    "y_pred_reg = reg.predict(X_test.iloc[:, :7].to_numpy())\n",
    "score_regression(y_rtest, y_pred_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42542 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6332BFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000001C6332BFA60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "42542/42542 [==============================] - 8s 181us/sample - loss: 72308.8231 - accuracy: 0.0097\n",
      "Epoch 2/20\n",
      "42542/42542 [==============================] - 6s 146us/sample - loss: 3158.4172 - accuracy: 0.0091\n",
      "Epoch 3/20\n",
      "42542/42542 [==============================] - 7s 155us/sample - loss: 7.5809 - accuracy: 0.0061\n",
      "Epoch 4/20\n",
      "42542/42542 [==============================] - 7s 156us/sample - loss: 5.0027 - accuracy: 0.0062\n",
      "Epoch 5/20\n",
      "42542/42542 [==============================] - 6s 151us/sample - loss: 4.0884 - accuracy: 0.0061\n",
      "Epoch 6/20\n",
      "42542/42542 [==============================] - 7s 161us/sample - loss: 3.5077 - accuracy: 0.0063\n",
      "Epoch 7/20\n",
      "42542/42542 [==============================] - 6s 140us/sample - loss: 2.9895 - accuracy: 0.0062\n",
      "Epoch 8/20\n",
      "42542/42542 [==============================] - 7s 165us/sample - loss: 2.7040 - accuracy: 0.0061\n",
      "Epoch 9/20\n",
      "42542/42542 [==============================] - 7s 169us/sample - loss: 2.4751 - accuracy: 0.0061\n",
      "Epoch 10/20\n",
      "42542/42542 [==============================] - 7s 156us/sample - loss: 2.2585 - accuracy: 0.0062\n",
      "Epoch 11/20\n",
      "42542/42542 [==============================] - 6s 138us/sample - loss: 2.0987 - accuracy: 0.0061\n",
      "Epoch 12/20\n",
      "42542/42542 [==============================] - 6s 136us/sample - loss: 1.9689 - accuracy: 0.0061\n",
      "Epoch 13/20\n",
      "42542/42542 [==============================] - 6s 150us/sample - loss: 1.8492 - accuracy: 0.0061\n",
      "Epoch 14/20\n",
      "42542/42542 [==============================] - 7s 169us/sample - loss: 1.7474 - accuracy: 0.0060\n",
      "Epoch 15/20\n",
      "42542/42542 [==============================] - 8s 186us/sample - loss: 1.6806 - accuracy: 0.0061\n",
      "Epoch 16/20\n",
      "42542/42542 [==============================] - 7s 167us/sample - loss: 1.6234 - accuracy: 0.0061\n",
      "Epoch 17/20\n",
      "42542/42542 [==============================] - 6s 148us/sample - loss: 1.5783 - accuracy: 0.0060\n",
      "Epoch 18/20\n",
      "42542/42542 [==============================] - 7s 159us/sample - loss: 1.5221 - accuracy: 0.0060\n",
      "Epoch 19/20\n",
      "42542/42542 [==============================] - 6s 141us/sample - loss: 1.4846 - accuracy: 0.0060\n",
      "Epoch 20/20\n",
      "42542/42542 [==============================] - 6s 143us/sample - loss: 1.4333 - accuracy: 0.0061\n",
      "Model Performance: MSE=1.5827\n"
     ]
    }
   ],
   "source": [
    "classifier_reg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(X_train_std.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.00000001, nesterov=False)\n",
    "classifier_reg.compile(optimizer=opt, loss='mean_squared_error', metrics=['accuracy'])\n",
    "classifier_reg.fit(X_train.to_numpy(), y_rtrain.to_numpy(), epochs=20)\n",
    "\n",
    "y_pred_reg = classifier_reg.predict(X_test.to_numpy())\n",
    "score_regression(y_rtest, y_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: MSE=3.2631\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10986691],\n",
       "       [ 0.23991095],\n",
       "       [-0.34694692],\n",
       "       ...,\n",
       "       [ 0.0956597 ],\n",
       "       [-0.04288995],\n",
       "       [ 0.15074253]], dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18622, 1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00190905 0.0575433  0.4032011  0.48665118 0.04683916 0.00385619]\n",
      " [0.00347802 0.07319234 0.39260495 0.47064242 0.05408182 0.00600046]\n",
      " [0.00484556 0.07138    0.38683167 0.45450288 0.06774367 0.01469623]\n",
      " ...\n",
      " [0.00111771 0.03497788 0.40016484 0.5337459  0.0289243  0.00106938]\n",
      " [0.00483004 0.07679388 0.38836578 0.45680222 0.06269442 0.01051365]\n",
      " [0.00092138 0.0323066  0.41052336 0.5202334  0.03445359 0.00156169]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_ffn = classifier_ffn.predict(X_test.to_numpy())\n",
    "print(y_pred_ffn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-stocks",
   "language": "python",
   "name": "ds-stocks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
